import csv
import numpy as np
import pandas as pd
import sys

class logistic_regression:
	def __init__(self, w = None, no_train = False):
		if not no_train:
			train_X = pd.read_csv('X_train', index_col = 'id').to_numpy(dtype=np.double)
			self.train_Y = pd.read_csv('Y_train', index_col = 'id').to_numpy(dtype=np.double).reshape(-1)
			self.train_X = np.hstack((np.ones((len(train_X), 1)), train_X, np.power(train_X[:, [0, 126, 210, 211, 212, 507]], 2), \
				np.power(train_X[:, [0, 126, 210, 211, 212, 507]], 3), np.power(train_X[:, [0, 126, 210, 211]], 4), \
				np.power(train_X[:, [0, 126, 210, 211]], 5), np.power(train_X[:, [0, 126, 210, 211]], 6), \
				np.power(train_X[:, [0, 126, 210, 211]], 7), np.power(train_X[:, [0, 126, 210, 211]], 8), \
				np.power(train_X[:, [0, 126, 210, 211]], 9)))
		self.w = w if w else np.array([-3.36405396e+00, 2.88458917e+00, -4.26815461e-02, 1.02720040e-01
								, -1.22649379e-01, 5.05076922e-03, -1.14923399e-02, -4.77298341e-02
								, -2.19176428e-03, 6.71545227e-02, -1.16392428e-01, -2.67630225e-02
								, 3.29366268e-02, -2.09927415e-02, 8.31774306e-02, 1.25964063e-02
								, 7.60359234e-03, 2.34158024e-03, -5.31477747e-03, 5.99252712e-03
								, -6.45441552e-02, 1.85186921e-02, 3.13850590e-02, -2.35807510e-03
								, 2.35653263e-02, 4.01548251e-02, 8.30358921e-03, -2.28517490e-02
								, 2.22950235e-02, -1.17175945e-02, 3.27696654e-02, -3.12473147e-02
								, -5.61626024e-02, -2.01982281e-02, 5.91213590e-02, -1.91116059e-02
								, 4.22498228e-02, 4.90808418e-02, -3.35810669e-02, 3.02054665e-02
								, 1.82376400e-02, -3.28283775e-02, 6.51206227e-02, 3.00366910e-02
								, 1.13635070e-02, -4.41359408e-02, -3.02558527e-02, -1.78000215e-02
								, -7.03179165e-03, 4.87862050e-03, 1.59272530e-02, 2.72356421e-02
								, -1.06254498e-02, -1.66854257e-02, -4.35959570e-02, -1.75213481e-02
								, 8.01480048e-03, -2.59183448e-02, 3.62036446e-02, 4.20662617e-03
								, -5.50630058e-03, 9.38485826e-03, -2.67844074e-03, -4.82271389e-02
								, 8.15549220e-02, -6.63872367e-02, -2.33457910e-03, -2.50672421e-02
								, 1.56187842e-02, 5.74366125e-02, 4.37818456e-02, -3.50187538e-02
								, -6.06901459e-02, -3.15955585e-02, -1.54856806e-01, -8.73620016e-02
								, -5.35553277e-02, -2.06729613e-02, -3.72261023e-02, -2.10911388e-02
								, 1.55283501e-03, 3.81434113e-02, -3.29003254e-02, -9.03792711e-02
								, -4.50161302e-02, -5.28458119e-02, -7.34680450e-02, -1.73811020e-01
								, 2.79883548e-02, 1.28358578e-02, -9.73112316e-02, 1.51034988e-03
								, 2.39616187e-03, 1.97988028e-02, -5.60844084e-02, -1.74940831e-01
								, -3.53081731e-03, -7.32412414e-02, -6.71540699e-02, 1.20064118e-01
								, -1.75783554e-02, -1.06254498e-02, -3.55300862e-02, -6.98025252e-03
								, 1.18496667e-01, 5.22332307e-02, 3.05009952e-02, 6.57452561e-02
								, 1.11354544e-03, 9.38485826e-03, -7.35083530e-02, -5.03185218e-02
								, -3.00231645e-01, -3.15211565e-02, 2.61283475e-01, -1.92125842e-01
								, -1.60305134e-01, 1.76297682e-01, 1.86430350e-01, -8.28206606e-03
								, -1.75651491e-01, 1.94159940e-01, -4.36363768e-02, -1.97040167e-01
								, -2.20210600e-01, -2.05674755e-01, -1.79701930e-01, -9.13896295e-01
								, -6.29583627e-02, -5.00172913e-02, 1.81375002e-01, 5.13830470e-02
								, 1.01191973e-01, 1.47907960e-01, 3.82293884e-02, -8.19203809e-02
								, -4.85556953e-02, 7.20211864e-02, 2.72356421e-02, 3.00366910e-02
								, -2.01982281e-02, -6.45441552e-02, 2.42740070e-02, 8.01480048e-03
								, -2.28517490e-02, 4.94847539e-02, -2.35807510e-03, 1.85186921e-02
								, 3.13850590e-02, -1.06254498e-02, -5.50630058e-03, 1.13635070e-02
								, -3.02558527e-02, 5.99252712e-03, 6.70167858e-03, 4.87862050e-03
								, 4.14960380e-02, -7.03179165e-03, 3.93259415e-02, -4.41359408e-02
								, 2.49307431e-02, 9.38485826e-03, -1.26368165e-01, -5.80399295e-03
								, -7.99830611e-02, -7.34680450e-02, 5.70083923e-02, -3.53081731e-03
								, -5.62992151e-02, 3.39610954e-02, -1.06254498e-02, -5.03294887e-02
								, -7.61506240e-02, -3.45929499e-02, -1.40839999e-01, 9.38485826e-03
								, 1.04095283e-01, 1.92007028e-02, 3.09533695e-02, 2.76479898e-02
								, -2.87180430e-02, -4.22149681e-02, 4.44822242e-02, -7.39751767e-03
								, -1.23495609e-02, -3.56811993e-02, 2.27173314e-02, -3.83597735e-02
								, -1.10414888e-01, 3.06093010e-02, -3.18104761e-02, 4.62364573e-02
								, 2.72785346e-01, -2.72785346e-01, -2.53340014e-02, 4.33708539e-02
								, 1.98082822e-02, -1.54122651e-02, -5.37246635e-03, 1.52046887e-03
								, 6.60029442e-03, 6.71545227e-02, -2.10653892e-02, 2.18711521e-03
								, -3.76635460e-01, 5.51680483e-02, 9.59633654e-02, 9.35526018e-03
								, 3.07976863e-02, -2.91482638e-02, 1.86990716e-02, 6.88858671e-01
								, -1.46880667e-01, 1.47788461e+00, -4.47341443e-01, 4.12727994e-02
								, 7.53918825e-02, -1.13334827e-02, 2.83607258e-01, 5.29392846e-02
								, 1.19820574e-02, 3.21433460e-02, -4.40182418e-03, -1.51467799e-02
								, -5.76638302e-02, 3.02027612e-03, 3.13244565e-02, -6.75012944e-03
								, -2.56856978e-02, 6.93135799e-03, -2.01900443e-02, -7.79774599e-03
								, -8.01634568e-03, -1.31652912e-02, -2.18094415e-03, 1.64342660e-02
								, 2.58220380e-02, -6.81890160e-03, 2.43395429e-02, 3.43440478e-02
								, 2.86884517e-04, 8.28715336e-03, -2.69783606e-02, 4.13594655e-02
								, -2.66070151e-02, -3.01564039e-02, -3.10660622e-02, -5.48780129e-02
								, 4.02200875e-02, -6.90038826e-03, -2.64446071e-02, 9.22864265e-03
								, -2.30124894e-02, 1.03896973e-03, -1.01750562e-02, 2.40248313e-02
								, -4.40182418e-03, 9.98161333e-03, 1.35045749e-02, 1.11093508e-02
								, -2.97449173e-02, 8.21054776e-03, 2.47994217e-02, 2.03671069e-02
								, -3.07188043e-02, 3.52551729e-03, -4.69964413e-02, 1.50036861e-04
								, -1.23273688e-02, 2.33845511e-02, -1.23278285e-03, 4.87340203e-03
								, 1.38468611e-02, -1.79915104e-02, -4.41408403e-02, -1.35312751e-02
								, 4.02750144e-02, 8.91468550e-03, 5.46748900e-01, -2.03752752e-01
								, -6.59152336e-02, -7.41246464e-03, -3.29959789e-02, 3.42709673e-02
								, 3.81354918e-02, -1.59717165e-02, -3.69687476e-02, 0.00000000e+00
								, 1.51711117e-02, -1.36298703e-01, -1.08421715e-01, -1.38954230e-02
								, -7.47994073e-02, -1.03951243e-01, -7.69886852e-03, -1.10920536e-01
								, 1.33662224e-02, 9.56188968e-03, -9.54863091e-02, 3.97220940e-02
								, -4.09973994e-02, -2.25545934e-01, -1.81174463e-01, 0.00000000e+00
								, 4.19584388e-02, -1.20909754e-01, -1.32864061e-02, -6.35181779e-02
								, -1.76220747e-02, -3.92557252e-02, -1.71108889e-02, -1.57073087e-02
								, 4.50975641e-02, -6.83601381e-02, -1.11351456e-02, -3.07532720e-02
								, -2.19955806e-01, -4.22545784e-01, -8.59492223e-02, -3.65044136e-02
								, 3.59871862e-02, 2.59146292e-01, -1.34302549e-01, -1.90526856e-01
								, 5.76020705e-03, 1.28786324e-02, 1.95984100e-01, -2.95355863e-02
								, 4.76670645e-02, 4.24929478e-03, -4.02109965e-02, -1.94162324e-02
								, -2.77167044e-02, -6.60677574e-04, 3.34331896e-03, 5.76020705e-03
								, 3.82054573e-02, 1.54198624e-03, 4.76670645e-02, 9.81580255e-03
								, -5.76638302e-02, -2.77167044e-02, 4.82739683e-02, 3.34331896e-03
								, 5.76020705e-03, -2.98707747e-02, 4.36357884e-03, 4.76670645e-02
								, 9.81580255e-03, 1.13397258e-02, -5.76638302e-02, -2.77167044e-02
								, -2.83576937e-02, 4.76670645e-02, 4.40182418e-03, 5.26911179e-02
								, -2.77167044e-02, 1.41059981e-02, -1.00366215e-02, 3.58382032e-01
								, 2.07023467e-01, -1.05789668e+00, -1.05873025e-01, 2.02918245e-01
								, -2.43477262e-01, 7.74610270e-03, -8.45352681e-03, 1.05718232e-01
								, -1.25488065e-01, -1.71336032e-01, 1.70448468e-02, 1.24403964e-02
								, 9.18956635e-03, 1.89953869e-04, 8.44860611e-02, 1.63803500e-02
								, -6.36754887e-03, -1.69519934e-02, 5.13856398e-02, 2.64769217e-02
								, 5.46095486e-03, 2.15760829e-02, 1.22823556e-01, 4.13496692e-03
								, 8.72757050e-02, -3.91284511e-01, -3.63268238e-02, 4.16367943e-02
								, 8.61909359e-03, 8.53464816e-02, -6.64454293e-02, 7.66916813e-03
								, 6.52021022e-02, -1.29754655e-02, 3.49668290e-03, 4.80910176e-02
								, 2.17073243e-02, 3.92652355e-02, 2.21633964e-02, -7.75624739e-02
								, -2.52359363e-02, -3.14818769e-03, -2.68426291e-02, -6.06372878e-02
								, 5.87818097e-02, 2.00244683e-01, -6.67435464e-02, 2.67413357e-02
								, 2.47729503e-02, 7.23097551e-03, -6.45840703e-02, -1.90117652e-02
								, -2.77696946e-02, -2.69809804e-02, 1.51316030e-02, -3.48229415e-02
								, 3.84514391e-02, -2.19402716e-02, -7.33099623e-02, 2.73907683e-02
								, 5.48783457e-02, -1.08699102e-01, 3.53393587e-03, -1.42208726e-01
								, 1.92002985e-02, -7.14129686e-02, -3.68258214e-02, 1.43628536e-02
								, 7.69913055e-02, -8.15586914e-02, 1.03630616e-01, 3.71076863e-03
								, 1.89216728e-03, -7.40087816e-02, 8.71619876e-02, 6.76893894e-03
								, -2.16583187e-02, 5.65091918e-02, 7.39577857e-03, -1.83330833e-02
								, 1.54251171e-02, -2.74696201e-02, -3.05331962e-02, -1.39187329e-01
								, 8.54710819e-03, 1.10796274e-02, 6.39884569e-02, -1.62419006e-02
								, -7.89158568e-02, -1.14121682e-02, -1.54186161e-03, -1.93835932e-01
								, -1.37662412e-01, -3.55853668e-02, -2.71236789e-02, 4.89384871e-02
								, -8.02262462e-03, -1.67617770e-02, 1.77920749e-01, -1.37502293e-01
								, -8.50863394e-02, -5.76565636e-03, -1.07024278e-01, -1.26608113e-01
								, -7.72993717e-03, -2.64492655e-02, 1.78112435e-02, -5.99974988e-02
								, -5.10431619e-02, -9.58009953e-02, -1.58764918e-01, -2.43973739e-01
								, 5.92745383e-02, -2.55479571e-01, -5.40818910e-02, -4.53432613e-02
								, -7.23802768e-02, -9.86610507e-02, -2.54007457e-02, -7.35960659e-02
								, -8.20134249e-02, -9.18536880e-02, -2.03585877e-01, -4.63716580e-02
								, -8.81766104e-02, -1.97891624e-02, -3.61410550e-02, 1.32984145e-02
								, -5.47933778e-02, -6.56428264e-02, -1.06894585e-02, -1.58719202e-01
								, -1.47564784e-02, -2.55547853e-02, -3.46003071e-02, 1.12901978e-01
								, 2.06334332e-01, -2.39661513e-01, 2.59386479e-01, -5.18877148e-02
								, -6.53359087e-02, 6.96780573e-02, -3.98941511e-02, -3.82485743e-04
								, 4.38083411e-02, 1.32009477e-01, -2.19978339e-01, 3.98941511e-02
								, -2.59890290e-02, 2.77167044e-02, -2.77167044e-02, -7.11145937e-01
								, 2.39746841e+00, 5.95422961e+00, -8.43800812e-01, -5.87226614e+00
								, 9.74138167e-01, -1.84990810e+00, -2.30614286e+00, -3.90106043e+00
								, 1.57280625e+00, 1.06014059e+01, -2.44652991e-01, -1.72572097e+00
								, -1.54794302e+00, -2.41588415e+00, 1.10364048e+00, 5.84307085e-01
								, 1.32085869e+00, -8.27282441e-01, -9.04416115e-01, 1.67653106e+00
								, 1.69717700e+00, -8.38855221e-02, -2.20905529e+00, 1.36224925e+00
								, 4.85454840e-01, 2.15492211e-01, -2.28954414e-02, 1.17116689e-01
								, -7.67032426e-01, 3.31665793e-01, 1.43832348e+00, -1.54444565e+00
								, -6.33261043e-01, 3.76414520e-01, 6.22972865e-02])
		self.validate = 0

	def get_validate_set(self, validate = 1000, seed = 64):
		self.validate = validate
		np.random.seed(seed)
		shuf = np.arange(len(self.train_X))
		np.random.shuffle(shuf)
		self.validate_X, self.validate_Y = self.train_X[:validate], self.train_Y[:validate]
		self.train_X, self.train_Y = self.train_X[validate:], self.train_Y[validate:]

	def get_result(self, test_file, result_file, normalize = True):
		output = [['id', 'label']]
		self.test_X = pd.read_csv(test_file, index_col = 'id').to_numpy(dtype=np.double)
		self.test_X = np.hstack((np.ones((len(self.test_X), 1)), self.test_X, np.power(self.test_X[:, [0, 126, 210, 211, 212, 507]], 2),\
			np.power(self.test_X[:, [0, 126, 210, 211, 212, 507]], 3), np.power(self.test_X[:, [0, 126, 210, 211]], 4), \
			np.power(self.test_X[:, [0, 126, 210, 211]], 5), np.power(self.test_X[:, [0, 126, 210, 211]], 6), \
			np.power(self.test_X[:, [0, 126, 210, 211]], 7), np.power(self.test_X[:, [0, 126, 210, 211]], 8), \
			np.power(self.test_X[:, [0, 126, 210, 211]], 9)))
		if normalize: self.normalize(is_train = False)
		result = np.round(self.sigmoid(np.matmul(self.test_X, self.w))).astype(np.int)
		for i in range(len(result)):
			output.append([i, result[i]])
		csv.writer(open(result_file, 'w')).writerows(output)

	def normalize(self, is_train = True):
		X = self.train_X if is_train else self.test_X
		col = np.arange(X.shape[1])
		X_mean = np.mean(X[..., col], 0).reshape(1, -1)
		X_std = np.std(X[..., col], 0).reshape(1, -1)
		X = (X[..., col] - X_mean) / (X_std + 1e-8)
		X = np.hstack((np.ones((len(X), 1)), X[..., col[1:]]))
		if is_train: self.train_X = X
		else: self.test_X = X

	def sigmoid(self, z):
		return np.clip(1. / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))

	def predict(self):
		self.train_sig = self.sigmoid(np.matmul(self.train_X, self.w))
		self.train_pre = np.round(self.train_sig).astype(np.int)
		if self.validate:
			self.validate_sig = self.sigmoid(np.matmul(self.validate_X, self.w))
			self.validate_pre = np.round(self.validate_sig).astype(np.int)

	def accuracy(self, verbose = False):
		t, v = None, None
		t = 1 - np.mean(np.abs(self.train_Y - self.train_pre))
		if verbose: print('acc on testing set:', t)
		if self.validate:
			v = 1 - np.mean(np.abs(self.validate_Y - self.validate_pre))
			if verbose: print('acc on validation set:', v)
		return t, v

	def cross_entropy(self, verbose = False):
		t, v = None, None
		t = (-np.dot(self.train_Y, np.log(self.train_sig)) - np.dot((1 - self.train_Y), np.log(1 - self.train_sig))) / len(self.train_Y)
		if verbose: print('loss on testing set:', t)
		if self.validate:
			v = (-np.dot(self.validate_Y, np.log(self.validate_sig)) - np.dot((1 - self.validate_Y), np.log(1 - self.validate_sig))) / len(self.validate_Y)
			if verbose: print('loss on validation set:', v)
		return t, v

	def gradient(self):
		self.predict()
		pre_error = self.train_Y - self.train_sig
		return -np.matmul(self.train_X.T, pre_error)

	def train(self, iter = 10, verbose = False):
		alpha, beta_1, beta_2, epsilon, v, s, self.w = 0.002, 0.9, 0.999, 1e-7, np.zeros(self.train_X.shape[1]), np.zeros(self.train_X.shape[1]), np.zeros(self.train_X.shape[1])
		for step in range(1, iter + 1):
			grad = self.gradient()
			v = beta_1 * v + (1. - beta_1) * grad
			s = beta_2 * s + (1. - beta_2) * np.power(grad, 2)
			v_hat = v / (1. - np.power(beta_1, step)) + (1 - beta_1) * grad / (1 - np.power(beta_1, step))
			s_hat = s / (1. - np.power(beta_2, step))
			self.w = self.w - alpha * v_hat / (np.sqrt(s_hat) + epsilon)
			if verbose:
				print(step)
				self.predict()
				self.accuracy()
				self.cross_entropy()
		self.predict()
		self.accuracy(verbose = True)
		self.cross_entropy(verbose = True)

model = logistic_regression(no_train = True)
#model.normalize()
#model.train(iter=15000)
model.get_result(sys.argv[1], sys.argv[2])
